
# Fine-tuning pre-trained models
The optimal hyperparameters we have searched are described here.  
Please keep in mind that different environments may produce different results from ours.

|model|comment|
|:----|:------|
|[BlueBERT-Base, Uncased, PubMed](./BlueBERT-Base-Uncased_P)|pretrained on PubMed abstracts. [1](#bluebert)|

## References
- <a id="bluebert"><a>Peng Y, Yan S, Lu Z. [Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets](https://arxiv.org/abs/1906.05474). In *Proceedings of the Workshop on Biomedical Natural Language Processing (BioNLP)*. 2019.
