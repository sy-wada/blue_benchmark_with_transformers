# Simultaneous PreTraining (SPT)

## Overview
This project provides sample code for generating training instances, proposed in our Simultaneous PreTraining (SPT). For demonstration purposes, we have prepared small and large corpora using PubMed Abstracts (1MB) and English Wikipedia (10MB), respectively. This serves as a foundation for text mining and data analysis within the biomedical literature domain as well as for general language understanding.

## Usage
Please refer to [`note.ipynb`](./note.ipynb) for detailed instructions on how to use the sample code. This includes how to execute commands or use the API, along with any necessary command line arguments or setup steps.

## About PubMed Data
The abstracts used in this project are extracted from PubMed, provided by the National Library of Medicine (NLM). The extracted data consists of citations from biomedical and health sciences literature, intended solely for research purposes.

## About Wikipedia Data
The excerpts used in this project are extracted from English Wikipedia. This data is intended to demonstrate the application of SPT on a large corpus.

### License
#### Our Code
Our sample code is provided under the [Apache-2.0 License](https://www.apache.org/licenses/LICENSE-2.0).

#### PubMed Data
For detailed information on the terms of use for PubMed data, please refer [here](https://www.nlm.nih.gov/databases/download/terms_and_conditions.html).

#### Wikipedia Data
Wikipedia content is available under the [Creative Commons Attribution-ShareAlike License](https://creativecommons.org/licenses/by-sa/3.0/). More detailed information on the reuse of Wikipedia content can be found [here](https://en.wikipedia.org/wiki/Wikipedia:Reusing_Wikipedia_content).
